from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import uvicorn
from fastapi.middleware.cors import CORSMiddleware
import os
import json
import re
import ast
import gc  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì„í¬íŠ¸
import weakref
import logging
import platform  # í”Œë«í¼ í™•ì¸ìš©

# Windowsì—ì„œëŠ” resource ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì¡°ê±´ë¶€ë¡œ ì„í¬íŠ¸
if platform.system() != "Windows":
    import resource  # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

# galaxy_chatbot.pyì˜ í•µì‹¬ ê¸°ëŠ¥ ì„í¬íŠ¸
from galaxy_chatbot import (
    cohere_embeddings, 
    hybrid_retriever, 
    text_vectorstore, 
    image_vectorstore, 
    llm, 
    AgentState
)

# SearchDocumentsTool í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•˜ì§€ ì•Šê³  í•„ìš”í•œ ê¸°ëŠ¥ë§Œ ì¬êµ¬í˜„
from galaxy_chatbot import client, np

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="ê°¤ëŸ­ì‹œ S25 ë§¤ë‰´ì–¼ ì±—ë´‡ API")

# CORS ì„¤ì • (Next.js ì•±ì—ì„œ API í˜¸ì¶œ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‹¤ì œ ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹… í•¨ìˆ˜
def log_memory_usage(location=""):
    try:
        if platform.system() != "Windows":
            usage = resource.getrusage(resource.RUSAGE_SELF)
            memory_mb = usage.ru_maxrss / 1024  # KBë¥¼ MBë¡œ ë³€í™˜
            logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ({location}): {memory_mb:.2f} MB")
        else:
            # Windows í™˜ê²½ì—ì„œëŠ” ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
            import psutil
            process = psutil.Process(os.getpid())
            memory_mb = process.memory_info().rss / 1024 / 1024  # bytesë¥¼ MBë¡œ ë³€í™˜
            logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ({location}): {memory_mb:.2f} MB")
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹… ì˜¤ë¥˜: {str(e)}")

# ì´ë¯¸ì§€ ê´€ë ¨ì„± ë¶„ì„ í•¨ìˆ˜ (SearchDocumentsToolì—ì„œ ì¶”ì¶œ)
def analyze_image_relevance(image_url, query_text):
    try:
        query_embedding = cohere_embeddings.embed_query(query_text)
        
        img_embedding = None
        try:
            resp = client.table("image_embeddings").select("embedding,metadata").eq("metadata->>image_url", image_url).execute()
            if resp and resp.data and len(resp.data) > 0:
                if 'embedding' in resp.data[0]:
                    embedding_str = resp.data[0]['embedding']
                    if isinstance(embedding_str, str):
                        try:
                            embedding_str = embedding_str.replace(" ", "")
                            img_embedding = ast.literal_eval(embedding_str)
                        except:
                            print(f"ì„ë² ë”© ë¬¸ìì—´ íŒŒì‹± ì‹¤íŒ¨: {embedding_str[:50]}...")
                            img_embedding = None
                    else:
                        img_embedding = embedding_str
                metadata = resp.data[0].get('metadata', {})
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ì„ë² ë”© ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            metadata = {}
        
        # ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚° (ê¸°ë³¸ ì ìˆ˜ 0.5)
        embedding_similarity = 0.5
        if img_embedding:
            try:
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                norm_q = np.linalg.norm(query_embedding)
                norm_img = np.linalg.norm(img_embedding)
                if norm_q > 0 and norm_img > 0:
                    embedding_similarity = np.dot(query_embedding, img_embedding) / (norm_q * norm_img)
                    # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
                    embedding_similarity = (embedding_similarity + 1) / 2
                    # numpy.float64ë¥¼ Python floatë¡œ ë³€í™˜
                    embedding_similarity = float(embedding_similarity)
            except Exception as e:
                print(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
                embedding_similarity = 0.5
        
        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ (URL íŒ¨í„´ ê¸°ë°˜)
        vertical_position = 0.5  # ê¸°ë³¸ê°’ (ì¤‘ê°„)
        if "top" in image_url.lower() or "upper" in image_url.lower():
            vertical_position = 0.2  # ìœ„ìª½
        elif "bottom" in image_url.lower() or "lower" in image_url.lower():
            vertical_position = 0.8  # ì•„ë˜ìª½
        elif "mid" in image_url.lower() or "middle" in image_url.lower():
            vertical_position = 0.5  # ì¤‘ê°„
        
        # ê¸°ë³¸ ê²°ê³¼ ì„¤ì •
        result = {
            "vertical_position": float(vertical_position),
            "relevance_score": float(embedding_similarity),
            "embedding_similarity": float(embedding_similarity),
            "metadata": metadata
        }
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del query_embedding
        del img_embedding
        
        return result
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return {
            "vertical_position": 0.5,  # ê¸°ë³¸ê°’
            "relevance_score": 0.5,  # ê¸°ë³¸ ê´€ë ¨ì„± ì ìˆ˜
            "embedding_similarity": 0.5
        }
    finally:
        # ëª…ì‹œì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()

# í˜ì´ì§€ì˜ ëª¨ë“  ì´ë¯¸ì§€ ê²€ìƒ‰ í•¨ìˆ˜
def get_all_page_images(page, query_text):
    try:
        # í•´ë‹¹ í˜ì´ì§€ì˜ ëª¨ë“  ì´ë¯¸ì§€ ê²€ìƒ‰
        resp = client.table("image_embeddings").select("*").eq("metadata->>page", str(page)).execute()
        
        if not resp or not resp.data or len(resp.data) == 0:
            # URL íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰ ì‹œë„
            resp = client.table("image_embeddings").select("*").ilike("metadata->>image_url", f"%p{page}%").execute()
            if not resp or not resp.data or len(resp.data) == 0:
                return []
        
        page_images = []
        for item in resp.data:
            if 'metadata' in item and item['metadata'] and 'image_url' in item['metadata']:
                img_url = item['metadata']['image_url']
                img_page = item['metadata'].get('page', page)
                
                # ì´ë¯¸ì§€ ê´€ë ¨ì„± ë¶„ì„
                img_analysis = analyze_image_relevance(img_url, query_text)
                
                # ì´ë¯¸ì§€ ì •ë³´ ì €ì¥
                image_info = {
                    "url": img_url,
                    "page": img_page,
                    "is_page_match": True,  # ê°™ì€ í˜ì´ì§€ì´ë¯€ë¡œ í•­ìƒ True
                    "text_similarity": float(img_analysis["embedding_similarity"]),
                    "vertical_position": float(img_analysis["vertical_position"]),
                    "relevance_score": float(img_analysis["relevance_score"]),
                    "score": float(0.7 * img_analysis["relevance_score"] + 0.3)  # ì ìˆ˜ ê³„ì‚° ë°©ì‹ ìˆ˜ì •
                }
                
                page_images.append(image_info)
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        page_images.sort(key=lambda x: x["relevance_score"], reverse=True)
        return page_images
        
    except Exception as e:
        print(f"í˜ì´ì§€ ì´ë¯¸ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return []

# ì§ì ‘ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ êµ¬í˜„ (_run í•¨ìˆ˜ ëŒ€ì²´)
def perform_search(query: str):
    """ê²€ìƒ‰ ê¸°ëŠ¥ ë˜í¼ í•¨ìˆ˜"""
    log_memory_usage("ê²€ìƒ‰ ì‹œì‘")
    
    normalized_query = query.strip().rstrip('.!?')
    
    debug_info = {
        "query": normalized_query,
        "results": [],
        "image_results": [],
        "page_info": {},
        "vertical_position": {}
    }
    
    try:
        # 1. í…ìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰
        docs = hybrid_retriever.invoke(normalized_query)
        
        if not docs:
            return "ë§¤ë‰´ì–¼ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", debug_info
        
        # 2. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
        page_info = {}
        page_numbers = []
        
        # í˜ì´ì§€ ë²ˆí˜¸ ì§ì ‘ ì¶”ì¶œ (ì¿¼ë¦¬ì—ì„œ)
        extracted_page = None
        page_pattern = re.search(r'(?:í˜ì´ì§€|page|p)?\s*(\d+)(?:í˜ì´ì§€|ìª½|page)?', normalized_query.lower())
        if page_pattern:
            extracted_page = page_pattern.group(1)
            debug_info["extracted_page"] = extracted_page
        
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í˜ì´ì§€ ì •ë³´ ìˆ˜ì§‘
        for rank, doc in enumerate(docs):
            doc_page = None
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
            if "page" in doc.metadata:
                doc_page = str(doc.metadata["page"])
            elif "category" in doc.metadata and doc.metadata["category"]:
                if isinstance(doc.metadata["category"], str) and "p" in doc.metadata["category"].lower():
                    page_matches = re.findall(r'p(\d+)', doc.metadata["category"].lower())
                    if page_matches:
                        doc_page = page_matches[0]
            
            if doc_page:
                # í˜ì´ì§€ë³„ ì ìˆ˜ ê³„ì‚° (ìˆœìœ„ì— ë°˜ë¹„ë¡€)
                page_score = 1.0 / (rank + 1)
                
                # ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œí•œ í˜ì´ì§€ì™€ ì¼ì¹˜í•˜ë©´ ì ìˆ˜ ê°€ì¤‘ì¹˜
                if extracted_page and doc_page == extracted_page:
                    page_score *= 1.5
                
                # í˜ì´ì§€ ì •ë³´ ì €ì¥
                if doc_page not in page_info:
                    page_info[doc_page] = {
                        "score": page_score,
                        "content": [doc.page_content]
                    }
                    page_numbers.append(doc_page)
                else:
                    page_info[doc_page]["score"] += page_score
                    page_info[doc_page]["content"].append(doc.page_content)
        
        # í˜ì´ì§€ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_pages = sorted([(page, info["score"]) for page, info in page_info.items()], 
                            key=lambda x: x[1], reverse=True)
        top_pages = [page for page, _ in sorted_pages]
        
        # ë””ë²„ê·¸ ì •ë³´ì— í˜ì´ì§€ ì •ë³´ ì¶”ê°€
        debug_info["page_numbers"] = top_pages
        debug_info["page_info"] = {page: " ".join(page_info[page]["content"])[:200] 
                                 for page in page_info}
        debug_info["page_scores"] = {page: float(page_info[page]["score"]) 
                                   for page in page_info}
        
        # 3. ì´ë¯¸ì§€ ê²€ìƒ‰ - ì£¼ìš” í˜ì´ì§€ì— ëŒ€í•œ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        all_images = []
        
        # ìƒìœ„ í˜ì´ì§€ì— ìˆëŠ” ëª¨ë“  ì´ë¯¸ì§€ ê²€ìƒ‰
        if top_pages:
            best_page = top_pages[0]
            # í•´ë‹¹ í˜ì´ì§€ì˜ ëª¨ë“  ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            page_images = get_all_page_images(best_page, normalized_query)
            
            if page_images:
                # ìµœëŒ€ 3ê°œê¹Œì§€ ì´ë¯¸ì§€ ì„ íƒ
                best_images = page_images[:3]
                debug_info["best_images"] = best_images
                
                # ê´€ë ¨ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©)
                result_texts = [doc.page_content for doc in docs[:3]]
                combined_text = " ".join(result_texts)
                
                # ê° ì´ë¯¸ì§€ë³„ í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                for img in best_images:
                    img_result = analyze_image_relevance(img["url"], combined_text)
                    img["text_relevance"] = float(img_result.get("relevance_score", 0.5))
                    img["relevance_score"] = float(img["text_relevance"])
        
        # 5. ìµœì¢… ê²°ê³¼ êµ¬ì„±
        result_text = ""
        
        # ì°¸ì¡° í˜ì´ì§€ ëª©ë¡ ì €ì¥
        reference_pages = []
        for doc in docs:
            if "page" in doc.metadata:
                page = str(doc.metadata["page"])
                if page not in reference_pages:
                    reference_pages.append(page)
        
        # í…ìŠ¤íŠ¸ ê²°ê³¼ êµ¬ì„±
        for i, doc in enumerate(docs[:5]):  # ìµœëŒ€ 5ê°œë§Œ ì‚¬ìš©
            result_text += f"ë‚´ìš©: {doc.page_content}\n"
            result_text += f"ì¹´í…Œê³ ë¦¬: {doc.metadata.get('category','ì—†ìŒ')}\n"
            result_text += f"í˜ì´ì§€: {doc.metadata.get('page','ì—†ìŒ')}\n"
            result_text += "\n"
            
            # ë””ë²„ê·¸ ì •ë³´ ì €ì¥
            debug_info["results"].append({
                "rank": i+1,
                "source": doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"),
                "score": float(doc.metadata.get("score", 0)),
                "page": doc.metadata.get("page", "ì—†ìŒ"),
                "category": doc.metadata.get("category", "ì—†ìŒ"),
                "section": doc.metadata.get("section", "ì—†ìŒ"),
                "preview": doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
            })
        
        # ì°¸ì¡° í˜ì´ì§€ ì •ë³´ ì¶”ê°€
        if reference_pages:
            reference_pages.sort()
            debug_info["reference_pages"] = reference_pages

        log_memory_usage("ê²€ìƒ‰ ì™„ë£Œ")
        
        # ê²°ê³¼ ë°˜í™˜ ì „ ì„ì‹œ ê°ì²´ ì •ë¦¬
        del docs
        
        return result_text, debug_info
    except Exception as e:
        import traceback
        debug_info["error"] = str(e)
        debug_info["traceback"] = traceback.format_exc()
        return "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: " + str(e), debug_info
    finally:
        # ëª…ì‹œì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()

# ìš”ì²­ ëª¨ë¸ ì •ì˜
class ChatRequest(BaseModel):
    message: str
    history: Optional[List[Dict[str, str]]] = None
    debug_mode: Optional[bool] = False

# ì‘ë‹µ ëª¨ë¸ ì •ì˜
class ChatResponse(BaseModel):
    answer: str
    context: str
    images: Optional[List[Dict[str, Any]]] = None
    debug_info: Optional[Dict[str, Any]] = None

# ê²€ìƒ‰ ìš”ì²­ ëª¨ë¸
class SearchRequest(BaseModel):
    query: str
    page_filter: Optional[str] = None
    limit: Optional[int] = 5

# ì´ë¯¸ì§€ ê²€ìƒ‰ ìš”ì²­ ëª¨ë¸
class ImageSearchRequest(BaseModel):
    query: str
    page: Optional[str] = None
    limit: Optional[int] = 3

# ì±—ë´‡ ëŒ€í™” ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        # ë˜í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì‹¤í–‰
        context, debug_info = perform_search(request.message)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        conversation_history = request.history if request.history else []
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        conversation_context = ""
        if conversation_history:
            conversation_context = "ì´ì „ ëŒ€í™” ë‚´ìš©:\n"
            for i, exchange in enumerate(conversation_history[-5:]):  
                conversation_context += f"[ëŒ€í™” {i+1}]\n"
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì¡° í™•ì¸ ë° ì˜¬ë°”ë¥¸ í•„ë“œ ì ‘ê·¼
                if "user" in exchange and "ai" in exchange:
                    # userì™€ ai í•„ë“œ í˜•ì‹ì¸ ê²½ìš° (streamlit_app.pyì—ì„œ ë³´ë‚´ëŠ” í˜•ì‹)
                    conversation_context += f"ì‚¬ìš©ì: {exchange.get('user', '')}\n"
                    conversation_context += f"ë„ìš°ë¯¸: {exchange.get('ai', '')}\n"
                elif "role" in exchange and "content" in exchange:
                    # roleê³¼ content í•„ë“œ í˜•ì‹ì¸ ê²½ìš°
                    if exchange["role"] == "user":
                        conversation_context += f"ì‚¬ìš©ì: {exchange.get('content', '')}\n"
                    elif exchange["role"] == "assistant":
                        conversation_context += f"ë„ìš°ë¯¸: {exchange.get('content', '')}\n"
                else:
                    # ê¸°íƒ€ ê²½ìš° (í‚¤ê°€ ì—†ëŠ” ê²½ìš°) - ê¸°ë³¸ ì²˜ë¦¬
                    user_msg = exchange.get('user', exchange.get('content', ''))
                    conversation_context += f"ì‚¬ìš©ì: {user_msg}\n"
                    ai_msg = exchange.get('ai', '')
                    if ai_msg:
                        conversation_context += f"ë„ìš°ë¯¸: {ai_msg}\n"
        
        # ì°¸ì¡° í˜ì´ì§€ ì¶”ì¶œ
        reference_pages = []
        if "reference_pages" in debug_info:
            reference_pages = debug_info["reference_pages"]
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
        ë‹¹ì‹ ì€ ì‚¼ì„± ê°¤ëŸ­ì‹œ S25ì˜ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” ê°€ìƒ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ë©°, í•„ìš”í•œ ê²½ìš° ë‹¨ê³„ë³„ ì•ˆë‚´ë¥¼ í•´ì£¼ì„¸ìš”.
        ê¸°ìˆ ì ì¸ ì •ë³´ë¿ë§Œ ì•„ë‹ˆë¼ ì‹¤ì œ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê³  ë„ì›€ì´ ë˜ëŠ” ì¡°ì–¸ë„ í•¨ê»˜ ì œê³µí•´ ì£¼ì„¸ìš”.
        ì¹œê·¼í•˜ê³  ëŒ€í™”í•˜ë“¯ ë‹µë³€í•˜ë˜, ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤.

        ëŒ€í™” ë§¥ë½ ìœ ì§€ì— ê´€í•œ ì•ˆë‚´:
        â€¢ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì§§ê±°ë‚˜ ëª¨í˜¸í•œ ê²½ìš°, ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•´ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        â€¢ "ì´ê²ƒì€?", "ì–´ë–»ê²Œ?", "ì™œ?" ê°™ì€ ì§§ì€ ì§ˆë¬¸ì´ë‚˜ ì´ì „ ë‹µë³€ì—ì„œ ì–¸ê¸‰ëœ ìš©ì–´ë‚˜ ê°œë…ì— ëŒ€í•œ ì§ˆë¬¸ì€ 
          ì´ì „ ëŒ€í™” ì£¼ì œì™€ ì—°ê²°ì§€ì–´ í•´ì„í•˜ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.
        â€¢ ì‚¬ìš©ìì˜ ì´ì „ ì§ˆë¬¸ë“¤ê³¼ ë‹¹ì‹ ì˜ ë‹µë³€ì„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ì—°ì†ì„± ìˆëŠ” ëŒ€í™”ë¥¼ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.
        â€¢ ì‚¬ìš©ìê°€ ìƒˆë¡œìš´ ì£¼ì œë¡œ ì „í™˜í•˜ì§€ ì•ŠëŠ” í•œ, ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•´ ì£¼ì„¸ìš”.

        {conversation_context}

        ì°¸ê³ í•  ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
        {context}

        ì‚¬ìš©ì ì§ˆë¬¸: {request.message}

        ìœ„ ì°¸ê³  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.  
        ë‚´ìš©ì´ ë¶€ì¡±í•˜ë‹¤ë©´ ê´€ë ¨ëœ ì¶”ê°€ íŒì´ë‚˜ ì¡°ì–¸ë„ í•¨ê»˜ ì œê³µí•˜ì„¸ìš”.
        """
        
        # LLM ì‘ë‹µ ìƒì„±
        response = llm.invoke(prompt)
        answer = response.content
        
        # ë§¤ë‰´ì–¼ í˜ì´ì§€ ì°¸ì¡° ë¬¸êµ¬ ì¶”ê°€ (ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
        if reference_pages and "ë§¤ë‰´ì–¼ì˜ ê´€ë ¨ ì„¹ì…˜" not in answer and "ë” ì•Œê³  ì‹¶ìœ¼ì‹œë©´" not in answer:
            reference_pages.sort()
            
            # ë¬¸ë§¥ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì•ˆë‚´ë¬¸ ìƒì„±
            if "ì„¤ì •" in request.message.lower() or "ë°©ë²•" in request.message.lower():
                reference_text = "\n\nğŸ’¡ ì´ ì„¤ì •ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹œë©´ ë§¤ë‰´ì–¼ì˜ ê´€ë ¨ ì„¹ì…˜ì„ ì°¸ê³ í•´ë³´ì„¸ìš”."
            elif "ê¸°ëŠ¥" in request.message.lower() or "ì‚¬ìš©" in request.message.lower():
                reference_text = "\n\nğŸ’¡ ì´ ê¸°ëŠ¥ì˜ ì¶”ê°€ ì˜µì…˜ê³¼ í™œìš©ë²•ì€ ë§¤ë‰´ì–¼ì—ì„œ ë” ìì„¸íˆ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            else:
                reference_text = "\n\nğŸ’¡ ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë§¤ë‰´ì–¼ì˜ ê´€ë ¨ ì„¹ì…˜ì„ ì°¸ê³ í•´ë³´ì„¸ìš”."
            
            answer += reference_text
        
        # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
        images = []
        if debug_info and "best_images" in debug_info and debug_info["best_images"]:
            images = debug_info["best_images"]
            
            # ì¶”ê°€ ì´ë¯¸ì§€ ì •ë³´ í…ìŠ¤íŠ¸ë„ ì‘ë‹µì— í¬í•¨
            img_info_text = "\n\n"
            
            for i, img in enumerate(images[:3]):  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ í‘œì‹œ
                relevance_score = float(img.get('text_relevance', img.get('relevance_score', 0)))
                match_score = float(img.get('score', 0))
                
                # ì´ë¯¸ì§€ ê°„ ê³µë°± ì²˜ë¦¬
                if i > 0:
                    img_info_text += "\n\n"
                
                # ì´ë¯¸ì§€ íƒœê·¸ì™€ URL - Next.jsê°€ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ì •í™•í•œ í˜•ì‹
                # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì´ê³  ì—¬ëŸ¬ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ğŸ‘‘ í‘œì‹œ ì¶”ê°€
                if i == 0 and len(images) > 1:
                    img_info_text += f"[ì´ë¯¸ì§€ {i+1}] ğŸ‘‘ í…ìŠ¤íŠ¸ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì´ë¯¸ì§€\n"
                else:
                    img_info_text += f"[ì´ë¯¸ì§€ {i+1}]\n"
                
                # URLì€ ë°˜ë“œì‹œ ë³„ë„ ì¤„ì— ë‹¨ë…ìœ¼ë¡œ ë°°ì¹˜ (Next.js ì¸ì‹ìš©)
                img_info_text += f"{img['url']}\n\n"
                
                # ë©”íƒ€ë°ì´í„°ëŠ” URL ë’¤ì— ë³„ë„ë¡œ í‘œì‹œ
                img_info_text += f"í˜ì´ì§€: {img.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
                img_info_text += f"ê´€ë ¨ì„± ì ìˆ˜: {relevance_score:.4f}, ë§¤ì¹­ ì ìˆ˜: {match_score:.4f}"
                
                # ì´ë¯¸ì§€ ê´€ë ¨ì„±ì— ëŒ€í•œ ì„¤ëª… ì¶”ê°€
                if relevance_score < 0.65 or match_score < 0.7:
                    img_info_text += " (ë‚®ì€ ê´€ë ¨ì„±)"
                elif relevance_score >= 0.8:
                    img_info_text += " (ë†’ì€ ê´€ë ¨ì„±)"
                else:
                    img_info_text += " (ì¤‘ê°„ ê´€ë ¨ì„±)"
            
            answer += img_info_text
        
        # ë””ë²„ê·¸ ëª¨ë“œê°€ ì•„ë‹ˆë©´ debug_infoë¥¼ Noneìœ¼ë¡œ ì„¤ì •
        if not request.debug_mode:
            debug_info = None
        
        return ChatResponse(
            answer=answer,
            context=context,
            images=[{
                "url": img["url"],
                "page": str(img.get("page", "")),
                "relevance_score": float(img.get("relevance_score", 0.5)),
                "match_score": float(img.get("score", 0.5)),
                "text_relevance": float(img.get("text_relevance", 0.5))
            } for img in images],
            debug_info=debug_info
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸
@app.post("/search")
async def search(request: SearchRequest):
    try:
        # ì¿¼ë¦¬ ì •ê·œí™”
        normalized_query = request.query.strip().rstrip('.!?')
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì‚¬ìš©
        docs = hybrid_retriever.invoke(normalized_query)
        
        # í˜ì´ì§€ í•„í„° ì ìš© (ì„ íƒ ì‚¬í•­)
        if request.page_filter:
            docs = [doc for doc in docs if doc.metadata.get("page") == request.page_filter]
        
        # ê²°ê³¼ ì œí•œ
        if request.limit and request.limit < len(docs):
            docs = docs[:request.limit]
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        for doc in docs:
            results.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": float(doc.metadata.get("score", 0))
            })
        
        return {"results": results}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")

# ì´ë¯¸ì§€ ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸
@app.post("/image-search")
async def image_search(request: ImageSearchRequest):
    try:
        # í˜ì´ì§€ ê¸°ë°˜ ì´ë¯¸ì§€ ê²€ìƒ‰
        if request.page:
            images = get_all_page_images(request.page, request.query)
            
            # ê²°ê³¼ ì œí•œ
            if request.limit and request.limit < len(images):
                images = images[:request.limit]
            
            return {"images": images}
        
        # ì¿¼ë¦¬ ê¸°ë°˜ ì´ë¯¸ì§€ ê²€ìƒ‰
        else:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = cohere_embeddings.embed_query(request.query)
            
            # ì´ë¯¸ì§€ ë²¡í„° ê²€ìƒ‰
            docs = image_vectorstore.similarity_search_by_vector(
                query_embedding,
                k=request.limit or 3
            )
            
            # ê²°ê³¼ êµ¬ì„±
            images = []
            for doc in docs:
                if 'image_url' in doc.metadata:
                    # ì´ë¯¸ì§€ ê´€ë ¨ì„± ë¶„ì„
                    img_analysis = analyze_image_relevance(
                        doc.metadata['image_url'], 
                        request.query
                    )
                    
                    images.append({
                        "url": doc.metadata['image_url'],
                        "page": doc.metadata.get('page', 'unknown'),
                        "relevance_score": float(img_analysis["relevance_score"]),
                        "vertical_position": float(img_analysis["vertical_position"]),
                        "metadata": doc.metadata
                    })
            
            return {"images": images}
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì´ë¯¸ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")

# ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
async def health_check():
    return {"status": "healthy", "message": "ê°¤ëŸ­ì‹œ S25 ì±—ë´‡ APIê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤."}

# ë£¨íŠ¸ ê²½ë¡œ í•¸ë“¤ëŸ¬ ì¶”ê°€
@app.get("/")
async def read_root():
    return {
        "message": "ê°¤ëŸ­ì‹œ S25 ë§¤ë‰´ì–¼ ì±—ë´‡ API",
        "version": "1.0.0",
        "endpoints": {
            "chat": "/chat - POST ìš”ì²­ìœ¼ë¡œ ì±—ë´‡ê³¼ ëŒ€í™”",
            "search": "/search - POST ìš”ì²­ìœ¼ë¡œ ë§¤ë‰´ì–¼ ê²€ìƒ‰",
            "image_search": "/image-search - POST ìš”ì²­ìœ¼ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰",
            "health": "/health - GET ìš”ì²­ìœ¼ë¡œ API ìƒíƒœ í™•ì¸"
        },
        "docs": "/docs - API ë¬¸ì„œ í™•ì¸"
    }

# ê° ìš”ì²­ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
@app.middleware("http")
async def clean_memory_after_request(request, call_next):
    response = await call_next(request)
    gc.collect()
    return response

# ì§ì ‘ ì‹¤í–‰ ì‹œ ì„œë²„ êµ¬ë™
if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í¬íŠ¸ ì½ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 8000 ì‚¬ìš©)
    port = int(os.environ.get("PORT", 8000))
    
    # ì„œë²„ ì‹œì‘
    uvicorn.run("app:app", host="0.0.0.0", port=port, reload=True)